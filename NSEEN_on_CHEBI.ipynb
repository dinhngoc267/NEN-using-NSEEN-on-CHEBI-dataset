{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NSEEN_on_CHEBI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAXJ-LKoeB6q",
        "outputId": "a934fffe-f363-4184-ecd9-b329e133f96d"
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/aa/ae64be5acaac9055329289e6bfd54c1efa28bfe792f9021cea495fe2b89d/tensorflow_gpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.19.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (50.3.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC2Kx1aWxKSU",
        "outputId": "81d7fa5e-16d1-4512-b6fd-dffc4e898d39"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 1.63 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqfAPZfOxbs2",
        "outputId": "411203f3-7708-4db9-b447-4861afd30c1a"
      },
      "source": [
        "pip install -U strsimpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: strsimpy in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "time: 2.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6R6WZQ5xc3Y",
        "outputId": "4e80dd03-82d5-49ea-fd98-e3ab5292c419"
      },
      "source": [
        "pip install faiss-cpu --no-cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.6/dist-packages (1.6.5)\n",
            "time: 2.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klP35paexeCT",
        "outputId": "a3bab349-f7b5-4f71-aec5-bf18f62b3c28"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/58/a4a65efcce5c81a67b6893ade862736de355a3a718af5533d30c991831ce/ipython_autotime-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (50.3.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.6.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.2.0\n",
            "time: 252 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp1KD9v4xfNe",
        "outputId": "abe74202-ef0d-4350-f394-96d2aaea33a0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from strsimpy.jaro_winkler import JaroWinkler\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import random\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import copy\n",
        "import re\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Embedding, Bidirectional, LSTM, InputLayer, GRU\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import math\n",
        "import csv\n",
        "import faiss\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.26 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du0IIfIGxgiD",
        "outputId": "c0279c89-7736-4fc3-867b-4407446a4809"
      },
      "source": [
        "def load_reference_dict(data_dir):\n",
        "  reference_dict = {};\n",
        "  tsv_file = open(data_dir); \n",
        "  read_tsv = csv.reader(tsv_file, delimiter=\"\\n\")\n",
        "\n",
        "  for row in read_tsv:\n",
        "    arr = row[0].split(': '); \n",
        "    values = [];\n",
        "    variant_names = arr[1].split('\\t');\n",
        "    \n",
        "    for name in variant_names:\n",
        "      values.append(name)  \n",
        "    reference_dict[arr[0]] = values\n",
        "\n",
        "  return reference_dict\n",
        "\n",
        "def char_to_int_dict(reference_dict, query_set): \n",
        "  text = \"\" \n",
        "  for key, values in reference_dict.items(): \n",
        "    for value in values: \n",
        "      text = text + value \n",
        "\n",
        "  for item in query_set:\n",
        "    text = text + item\n",
        "\n",
        "  vocab = sorted(set(text)) \n",
        "  return dict((c,i+1) for i,c in enumerate(vocab))\n",
        "\n",
        "def get_maxlen_sequence(reference_dict): \n",
        "  maxlen=0 \n",
        "  t = \"\"\n",
        "  for key, values in reference_dict.items(): \n",
        "    for value in values: \n",
        "      if maxlen < len(value): \n",
        "        maxlen = len(value) \n",
        "        t = value\n",
        "  return (maxlen,t)\n",
        "\n",
        "def split_character(word): \n",
        "  return [char for char in word]\n",
        "\n",
        "#convert char sequence to int sequence in reference set and pad sequence\n",
        "def embedding_dataset(data,maxlen, char_to_int): \n",
        "  idx = 0 \n",
        "  for values in data: \n",
        "    value_1 = [char_to_int[c] for c in values[0]]; \n",
        "    value_2 = [char_to_int[c] for c in values[1]];\n",
        "    data[idx] = pad_sequences([value_1, value_2],maxlen,padding= 'post') \n",
        "    idx += 1; \n",
        "  return data;\n",
        "\n",
        "def create_random_list(list_size, population):\n",
        "  sample = [] \n",
        "  count = 0 \n",
        "  while count < list_size: \n",
        "    index = np.random.randint(0, len(population), size=1) \n",
        "    if population[index[0]] != 0: \n",
        "      sample.append(index[0]) \n",
        "      population[index[0]] = 0 \n",
        "      count += 1\n",
        "\n",
        "  return sample, population\n",
        "\n",
        "#Prepare 3 sub dataset\n",
        "#1.Semantic\n",
        "\n",
        "def create_sematic_pairs(reference_dict):\n",
        "  # Create positive sematic pairs\n",
        "  # The idea is pairwise crossproduct for terms have the same ID.\n",
        "  sematic_pairs = []; \n",
        "  labels = [];\n",
        "\n",
        "  for key,values in reference_dict.items(): \n",
        "    lst = []; n = len(values); \n",
        "    for pair in combinations(values,2):\n",
        "        if pair[0][0:120] != pair[1][0:120]: \n",
        "          sematic_pairs.append(pair);\n",
        "          labels.append((1,1))\n",
        "  print(\"There are \",len(sematic_pairs),\" positive pairs in sematic set\")\n",
        "\n",
        "  #Negative sematic pairs\n",
        "  # The idea is choose random n pair terms have different IDs from each other. \n",
        "  count = 0;\n",
        "  \n",
        "  ls = random.choices(list(reference_dict.items()),k= (len(reference_list)))\n",
        "  \n",
        "  for i in range(0, len(ls)-1):\n",
        "    if ls[i][0] != ls[i+1][0]:\n",
        "      term_1 = random.choice(ls[i][1])\n",
        "      term_2 = random.choice(ls[i+1][1])\n",
        "      if term_1[0:120] != term_2[0:120]:\n",
        "        sematic_pairs.append((term_1, term_2));\n",
        "        labels.append((0,0));\n",
        "        count += 1\n",
        "\n",
        "  print(\"There are \",count ,\" negative pairs in sematic set\") \n",
        "  return sematic_pairs, labels\n",
        "\n",
        "\n",
        "def create_syntactic_pairs(reference_dict):\n",
        "\n",
        "#Create Syntatic Variations\n",
        "  syntactic_pairs = []; \n",
        "  labels = []; \n",
        "  \n",
        "  #Same name syntactic\n",
        "  for key, values in reference_dict.items(): \n",
        "    for value in values:\n",
        "      modified_str = copy.deepcopy(value);\n",
        "      #1. Find all sequence contain characters which are not alphanumerical. If '-' character then replace with ' '. Else remove.     \n",
        "      if '-' in value:        \n",
        "        modified_str = modified_str.replace('-', ' ');\n",
        "      #2. Convert to lower cases.      \n",
        "      if value.islower() == False: #and value.lower() not in values:\n",
        "        modified_str = modified_str.lower();\n",
        "      #3. Remove 's / s in  tions -> tion \n",
        "      if \"'s\" in value:\n",
        "        modified_str = modified_str.replace(\"'s\",\"\");\n",
        "      if \"'\" in value:\n",
        "        modified_str = modified_str.replace(\"'\",\"\");        \n",
        "      \n",
        "      if modified_str != value and modified_str not in values:\n",
        "        syntactic_pairs.append((value, modified_str));\n",
        "        #print(values[0],',',modified_str);\n",
        "  \n",
        "  #Calculate the similarity between pair \n",
        "  jarowinkler = JaroWinkler()\n",
        "  for i,pair in enumerate(syntactic_pairs): \n",
        "    similarity = jarowinkler.similarity(pair[0], pair[1])\n",
        "    labels.append((similarity,1));\n",
        "\n",
        "  print(\"There are\", len(syntactic_pairs), \"syntactic pairs\") \n",
        "  return syntactic_pairs, labels\n",
        " \n",
        "\n",
        "def cosine_distance(vects): \n",
        "  x,y=vects  \n",
        "  t1_norm = tf.nn.l2_normalize(x, axis = 1)\n",
        "  t2_norm = tf.nn.l2_normalize(y, axis = 1)\n",
        "\n",
        "  cosine =  -tf.losses.cosine_similarity(t1_norm, t2_norm, axis = 1)\n",
        "  return 1-cosine\n",
        "\n",
        "def cos_dist_output_shape(shapes): \n",
        "  shape1, shape2 = shapes \n",
        "  return (shape1[0], 1)\n",
        "\n",
        "def contrastive_loss(y, d):\n",
        "  #d: distance; y: labels\n",
        "  margin = 1\n",
        "  return K.mean((y) * K.square(d) + (1-y)*K.square(K.maximum(margin - d, 0)))\n",
        "\n",
        "def create_base_network(input_shape): \n",
        "  model = Sequential()\n",
        "  #model.add(GRU(64,recurrent_dropout=0,return_sequences=True,  input_shape=input_shape, activation = 'tanh',use_bias = True, return_state=False,unroll = False,recurrent_activation = 'sigmoid',));\n",
        "  model.add(Bidirectional(LSTM(64,return_sequences=True, input_shape= input_shape)))\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  return model\n",
        "\n",
        "def shuffle_data(data, labels): \n",
        "  indices = np.arange(data.shape[0]) \n",
        "  np.random.shuffle(indices) \n",
        "  data = data[indices]\n",
        "  labels = labels[indices] \n",
        "  return data, labels\n",
        "\n",
        "\n",
        "#Define Siamese network\n",
        "def Siamese_network(input_shape, dataset, labels, weights_file, save_dir): \n",
        "  base_network = create_base_network(input_shape);\n",
        "  input_a = Input(shape=input_shape);\n",
        "  input_b = Input(shape=input_shape);\n",
        "\n",
        "  processed_a = base_network(input_a);\n",
        "  processed_b = base_network(input_b);\n",
        "\n",
        "  distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([processed_a, processed_b]);\n",
        "  model = Model([input_a, input_b], distance);\n",
        " \n",
        "  history = None;\n",
        "  if weights_file ==\"\":\n",
        "    model.compile(loss=contrastive_loss, optimizer=Adam(learning_rate=0.0001))\n",
        "\n",
        "    pop = [];\n",
        "    for i in range(0, len(training_set)):\n",
        "      pop.append(i+1);\n",
        "\n",
        "    count = 0;\n",
        "    max_count = np.ceil(len(training_set)/1024);\n",
        "\n",
        "    filepath=\"/content/drive/My Drive/Colab Notebooks/NSEEN-weights-model-23.07.20-train_siamese_net-4_BiLSTM_layers-{epoch:02d}-{loss:.6f}.hdf5\";\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1 , mode='min');\n",
        "    callbacks_list = [checkpoint, myCallback(\"\")];\n",
        "\n",
        "    #train_gen = generator(training_set, training_labels, pop, count,max_count)  \n",
        "    #history = model.fit_generator(train_gen,\n",
        "      #            steps_per_epoch =  np.ceil(len(training_set)/1024),\n",
        "     #             epochs=5,\n",
        "      #            callbacks=callbacks_list)\n",
        "    model.fit([training_set[:,0],training_set[:,1]], training_labels[:,0], epochs=5, batch_size=1024, callbacks=callbacks_list);\n",
        "  else: \n",
        "    model.load_weights(weights_file) ;\n",
        "    model.compile(loss=contrastive_loss, optimizer=Adam(learning_rate=0.0001)) \n",
        "  return model, history;\n",
        "import keras\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def __init__(self, save_dir):\n",
        "    self.max_acc = 0;\n",
        "    self.save_dir = save_dir;\n",
        "  # Khi kết thúc mỗi epoch cần embed lại reference set và predict lại evaluate data (biểu diễn lại chuỗi dựa vào mô hình học)\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    model_base = self.model.layers[2];\n",
        "    reference_embeddings = model_base.predict(reference_vecs, batch_size=512);\n",
        "    predicted_dev = model_base.predict(dev_set,batch_size=512);\n",
        "\n",
        "    index = faiss.IndexFlatIP(128)   # build the index\n",
        "\n",
        "    prev = copy.deepcopy(predicted_dev);\n",
        "    reference = copy.deepcopy(reference_embeddings);\n",
        "    faiss.normalize_L2(prev)  \n",
        "    faiss.normalize_L2(reference)\n",
        "\n",
        "    index.add(reference)\n",
        "    D, I = index.search(prev,10);\n",
        "\n",
        "    #index.add(reference_embeddings) \n",
        "\n",
        "    #D,I = index.search(predicted_dev[0:], 10) # Trả về index trong mảng 1 chiều sau khi đã flatten. \n",
        "                                                      # Cần biết vị trí của vector trong tập reference. -> Dựa vào reference_vecs_pos\n",
        "    result = [];\n",
        "    list_id = [];\n",
        "    for i in range(0,len(predicted_dev)):\n",
        "      tmp = [];\n",
        "      lst = [];\n",
        "      for j in range(0,10):\n",
        "        idx = I[i][j]      # I là mảng 2 chiều. vì chỉ đi tìm neighbors của 1 vector nên lấy dòng đầu tiên và cột thứ j\n",
        "        tmp.append(reference_list[idx]);\n",
        "        lst.append(reference_list_idx[idx][1]);\n",
        "      result.append(tmp);\n",
        "      list_id.append(lst)\n",
        "    \n",
        "    n_hit_1 = 0; n_hit_3 = 0; n_hit_5 = 0; n_hit_10 = 0;\n",
        "\n",
        "    for i in range(0,len(result)):\n",
        "      flag = False;\n",
        "      for j in range(0,1):\n",
        "        if str(list_id[i][j]) in evaluate_dev[i, 1]:\n",
        "          flag = True;\n",
        "          n_hit_1 += 1;\n",
        "          n_hit_3 += 1;\n",
        "          n_hit_5 += 1;\n",
        "          n_hit_10 += 1;\n",
        "          break;\n",
        "      if flag == False:\n",
        "        for j in range(1,3):\n",
        "          if str(list_id[i][j]) in evaluate_dev[i, 1]:\n",
        "            flag = True;\n",
        "            n_hit_3 += 1;\n",
        "            n_hit_5 += 1;\n",
        "            n_hit_10 += 1;\n",
        "            break;\n",
        "\n",
        "        if flag == False:\n",
        "          for j in range(3,5):\n",
        "            if str(list_id[i][j]) in evaluate_dev[i, 1]:\n",
        "              flag = True;\n",
        "              n_hit_5 += 1;\n",
        "              n_hit_10 += 1;\n",
        "              break;\n",
        "          \n",
        "          if flag == False:\n",
        "            for j in range(5,10):\n",
        "              if str(list_id[i][j]) in evaluate_dev[i, 1]:\n",
        "                flag = True;\n",
        "                n_hit_10 += 1;\n",
        "                break;\n",
        "\n",
        "    print(n_hit_1, n_hit_3, n_hit_5, n_hit_10);\n",
        "    avg_acc = (float((n_hit_1/len(predicted_dev))) + float((n_hit_3/len(predicted_dev))) + float((n_hit_5/len(predicted_dev))) + float((n_hit_10/len(predicted_dev))))/4;\n",
        "    print('Average Accuray:', avg_acc*100,'%');\n",
        "\n",
        "\n",
        "def flatten_dict(referece_dict): \n",
        "  reference_list = []; \n",
        "  reference_list_idx = [] # Save the index of term. \n",
        "  idx = 0; \n",
        "  for key, values in reference_dict.items(): \n",
        "    for value in values:\n",
        "      reference_list.append(value) \n",
        "      reference_list_idx.append((idx, key)); \n",
        "      idx += 1;\n",
        "\n",
        "  return (reference_list, reference_list_idx);\n",
        "\n",
        "\n",
        "\n",
        "#cấu trúc dữ liệu: training_set = str, [normalization_id],\n",
        "def create_evaluate_data(dir): \n",
        "  evaluate_data = [];\n",
        "\n",
        "  tsv_file = open(dir); \n",
        "  read_tsv = csv.reader(tsv_file, delimiter=\"\\n\")\n",
        "\n",
        "  for row in read_tsv:\n",
        "    arr = row[0].split(': '); \n",
        "    values = [];\n",
        "    variant_names = arr[1].split('\\t');\n",
        "    \n",
        "    for name in variant_names:\n",
        "      values.append(name)  \n",
        "    evaluate_data.append((arr[0],values))\n",
        "  return evaluate_data\n",
        "\n",
        "def generator(data, labels, pop, count, max_count, batch_size=1024):\n",
        "  while 1:\n",
        "    n = batch_size;    \n",
        "    if count == max_count-1:\n",
        "      n = len(data) - (max_count-1)*1024;    \n",
        "    indices, pop = create_random_list(n, pop);\n",
        "    \n",
        "    if count == max_count - 1:\n",
        "      pop = [];\n",
        "      for i in range(0, len(training_set)):\n",
        "        pop.append(i+1);\n",
        "      count = -1;\n",
        "\n",
        "    np.random.shuffle(indices);\n",
        "    samples = data[indices];\n",
        "    target = labels[indices];\n",
        "    count += 1;\n",
        "    yield [samples[:,0],samples[:,1]], target[:,0];\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 981 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRL7R5lTALAd"
      },
      "source": [
        "## CHEBI CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA0SgNGtzbNR",
        "outputId": "15e56b97-6828-4949-f63d-80b6ef6d48b6"
      },
      "source": [
        "# ===== LOAD BỘ THAM CHIẾU ĐÃ ĐƯỢC XỬ LÝ VÀ TẠO DỮ LIỆU HUẤN LUYỆN ===========\r\n",
        "\r\n",
        "#Create reference dictionary of entities from the xml files.\r\n",
        "#The value of each key is a sequence of characters. \r\n",
        "print(\"Creating reference dictionary...\")\r\n",
        "reference_dict = load_reference_dict(\"/content/drive/My Drive/chebi_reference_set.tsv\")\r\n",
        "print(\"Size of reference: \", len(reference_dict))\r\n",
        "evaluate_dev = create_evaluate_data('/content/drive/My Drive/chebi_query_set.tsv');\r\n",
        "evaluate_dev = np.array(evaluate_dev, dtype='object');\r\n",
        "dev_set = evaluate_dev[:,0]\r\n",
        "\r\n",
        "char_to_int = char_to_int_dict(reference_dict, dev_set)\r\n",
        "n = len(char_to_int)\r\n",
        "(reference_list, reference_list_idx) = flatten_dict(reference_dict)\r\n",
        "reference_vecs = copy.deepcopy(reference_list)\r\n",
        "\r\n",
        "#Creating 2 training set.\r\n",
        "print(\"Creating training set including: \")\r\n",
        "print(\"1. Creating sematic set...\")\r\n",
        "(sematic_pairs, sematic_labels) = create_sematic_pairs(reference_dict)\r\n",
        "#print(\"2. Creating syntactic set...\")\r\n",
        "#(syntactic_pairs, syntactic_labels) = create_syntactic_pairs(reference_dict)\r\n",
        "#Now, we need to transform the sequence of characters into vector of integers so that we can feed into the model.\r\n",
        "#Here, I use char to index method to embedd the sequence into vector. \r\n",
        "print(\"Concatenating sematic and syntactic sets...\")\r\n",
        "dataset = sematic_pairs# + syntactic_pairs;\r\n",
        "labels = sematic_labels# + syntactic_labels;\r\n",
        "\r\n",
        "print(\"Embedding the dataset into vec of integers...\")\r\n",
        "(maxlen, value) = get_maxlen_sequence(reference_dict)\r\n",
        "\r\n",
        "maxlen = 120\r\n",
        "idx = 0;\r\n",
        "for value in reference_vecs: \r\n",
        "  reference_vecs[idx] = [char_to_int[c] for c in value];    \r\n",
        "  idx +=1;\r\n",
        "\r\n",
        "reference_vecs = pad_sequences(reference_vecs,maxlen,padding= 'post')\r\n",
        "reference_vecs = reference_vecs/n\r\n",
        "reference_vecs = np.reshape(reference_vecs,reference_vecs.shape + (1,))\r\n",
        "\r\n",
        "#Tạo tập query\r\n",
        "\r\n",
        "idx = 0;\r\n",
        "for value in dev_set: \r\n",
        "  dev_set[idx] = [char_to_int[c] for c in value];    \r\n",
        "  idx +=1;\r\n",
        "dev_set = pad_sequences(dev_set, maxlen, padding='post')\r\n",
        "dev_set = dev_set/n\r\n",
        "dev_set = np.reshape(dev_set, dev_set.shape + (1,))\r\n",
        "\r\n",
        "training_set = sematic_pairs #+ syntactic_pairs;\r\n",
        "training_labels = sematic_labels #+ syntactic_labels;\r\n",
        "\r\n",
        "training_set = embedding_dataset(training_set, maxlen,char_to_int)\r\n",
        "training_set = np.array(training_set, dtype='float32');\r\n",
        "training_set = training_set/n\r\n",
        "#training_set = training_set / maxlen;\r\n",
        "training_labels = np.array(training_labels);\r\n",
        "print(\"Shuffle data...\")\r\n",
        "(training_set, training_labels) = shuffle_data(training_set, training_labels);\r\n",
        "\r\n",
        "print(\"Reshape data into the shape of (samples, timestep, features)...\")\r\n",
        "training_set = np.reshape(training_set, training_set.shape + (1,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating reference dictionary...\n",
            "Size of reference:  58597\n",
            "Creating training set including: \n",
            "1. Creating sematic set...\n",
            "There are  938810  positive pairs in sematic set\n",
            "There are  273792  negative pairs in sematic set\n",
            "Concatenating sematic and syntactic sets...\n",
            "Embedding the dataset into vec of integers...\n",
            "Shuffle data...\n",
            "Reshape data into the shape of (samples, timestep, features)...\n",
            "time: 48.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLMqqx-mDNLM",
        "outputId": "5ca4f14e-f3e9-4ad8-d7df-2a140337e943"
      },
      "source": [
        "len(reference_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "273798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3.24 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4mz87DCjQr0",
        "outputId": "94112f23-2468-4338-ca2d-2d8c3ff47a31"
      },
      "source": [
        "# Train M / Load M\n",
        "input_shape = training_set.shape[2:]\n",
        "base_network = create_base_network(input_shape);\n",
        "input_a = Input(shape=input_shape);\n",
        "input_b = Input(shape=input_shape);\n",
        "\n",
        "processed_a = base_network(input_a);\n",
        "processed_b = base_network(input_b);\n",
        "\n",
        "distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([processed_a, processed_b]);\n",
        "model = Model([input_a, input_b], distance);\n",
        "\n",
        "model.compile(loss=contrastive_loss, optimizer=Adam(learning_rate=0.0001))\n",
        "training_set = K.cast_to_floatx(training_set)\n",
        "training_labels = K.cast_to_floatx(training_labels)\n",
        "\n",
        "filepath=\"/content/drive/My Drive/Colab Notebooks/NSEEN-weights-model-05.08-HNM-2-Adam-epoch:{epoch:02d}-{loss:.6f}.hdf5\";\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1 , mode='min');\n",
        "callbacks_list = [checkpoint, myCallback(\"\")];\n",
        "model.fit([training_set[:,0], training_set[:,1]],training_labels[:,0], epochs=1,batch_size=1024, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1185/1185 [==============================] - 408s 326ms/step - loss: 0.1620\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN-weights-model-05.08-HNM-2-Adam-epoch:01-0.157129.hdf5\n",
            "491 541 546 550\n",
            "Average Accuray: 41.04938271604938 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c9de1d4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "stream",
          "text": [
            "time: 7min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwAbUQpHIxKq",
        "outputId": "a2b41dcd-93ee-4300-beb4-1d936fe4eee7"
      },
      "source": [
        "model_base = model.layers[2];\r\n",
        "reference_embeddings = model_base.predict(reference_vecs, batch_size=512);\r\n",
        "predicted_dev = model_base.predict(dev_set,batch_size=512);\r\n",
        "\r\n",
        "index = faiss.IndexFlatIP(128)   # build the index\r\n",
        "\r\n",
        "prev = copy.deepcopy(predicted_dev);\r\n",
        "reference = copy.deepcopy(reference_embeddings);\r\n",
        "faiss.normalize_L2(prev)  \r\n",
        "faiss.normalize_L2(reference)\r\n",
        "\r\n",
        "index.add(reference)\r\n",
        "D, I = index.search(prev,10);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 23.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IL0jbtwI2UK",
        "outputId": "6f6be721-33f3-4fa6-c93d-b4daa5122e59"
      },
      "source": [
        "D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0000001 , 0.9998049 , 0.9994435 , ..., 0.99848306, 0.99846566,\n",
              "        0.99837995],\n",
              "       [0.99999994, 0.9995487 , 0.9994616 , ..., 0.9989927 , 0.9987002 ,\n",
              "        0.9986995 ],\n",
              "       [1.0000001 , 0.99999833, 0.99999833, ..., 0.999983  , 0.99997956,\n",
              "        0.99997956],\n",
              "       ...,\n",
              "       [0.99999887, 0.99999887, 0.9999898 , ..., 0.99995476, 0.9999503 ,\n",
              "        0.99994785],\n",
              "       [0.99999666, 0.99999267, 0.9999875 , ..., 0.9999278 , 0.9999083 ,\n",
              "        0.9998673 ],\n",
              "       [0.99997663, 0.9999703 , 0.9999635 , ..., 0.9999386 , 0.9999349 ,\n",
              "        0.9999341 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "text": [
            "time: 4.45 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi63J5hLoYA1",
        "outputId": "212985cd-ada5-4e3d-baea-1d46591b7b2c"
      },
      "source": [
        "n_negative = 270000\r\n",
        "n_times = 1\r\n",
        "neg_pair = [];\r\n",
        "while n_times < 5:\r\n",
        "  print('Hard negative mining ', n_times, ':')\r\n",
        "  \r\n",
        "  tmp_neg_pair = []\r\n",
        "  model_base = model.layers[2];\r\n",
        "  print('Embedding reference...')\r\n",
        "  reference_embeddings = model_base.predict(reference_vecs, batch_size=1024);\r\n",
        "  print('Done!')\r\n",
        "\r\n",
        "  print('Searching...')\r\n",
        "  index = faiss.IndexFlatIP(128)   # build the index\r\n",
        "  faiss.normalize_L2(reference_embeddings);\r\n",
        "  index.add(reference_embeddings);\r\n",
        "  D, I = index.search(reference_embeddings,3);\r\n",
        "  print('Done')\r\n",
        "  hard_neg_pairs = [];\r\n",
        "  hard_neg_labels = [];\r\n",
        "\r\n",
        "  for i in range(0, len(reference_embeddings)):\r\n",
        "    for j in range(0,3):\r\n",
        "      closest_idx = I[i][j];\r\n",
        "\r\n",
        "      closest_meddra_id = reference_list_idx[closest_idx][1];\r\n",
        "      current_meddra_id = reference_list_idx[i][1];\r\n",
        "\r\n",
        "      if current_meddra_id != closest_meddra_id and reference_list[i] != reference_list[closest_idx]:\r\n",
        "        if i < closest_idx:\r\n",
        "          tmp_neg_pair.append((i,closest_idx))\r\n",
        "        else:\r\n",
        "          tmp_neg_pair.append((closest_idx, i))\r\n",
        "\r\n",
        "  tmp_neg_pair = set(tmp_neg_pair)-set(neg_pair)\r\n",
        "  for pair in tmp_neg_pair:\r\n",
        "    neg_pair.append(pair)\r\n",
        "\r\n",
        "  hard_neg_pairs = []\r\n",
        "  hard_neg_labels = []\r\n",
        "\r\n",
        "  for pair in tmp_neg_pair:\r\n",
        "    hard_neg_pairs.append((reference_list[pair[0]], reference_list[pair[1]]));\r\n",
        "    hard_neg_labels.append((0,0))\r\n",
        "  print('Found ', len(hard_neg_pairs),' hard negative pairs');\r\n",
        "  n_negative += len(hard_neg_pairs)\r\n",
        "  n_times += 1\r\n",
        "\r\n",
        "  print('Concatenating hard negative samples to training samples...')\r\n",
        "  hard_neg_pairs = embedding_dataset(hard_neg_pairs, maxlen,char_to_int)\r\n",
        "  hard_neg_pairs = np.array(hard_neg_pairs, dtype='float32')\r\n",
        "  hard_neg_pairs = np.reshape(hard_neg_pairs, hard_neg_pairs.shape + (1,))\r\n",
        "  hard_neg_pairs = hard_neg_pairs/n\r\n",
        "\r\n",
        "  training_set = np.append(training_set,hard_neg_pairs, axis=0)\r\n",
        "  hard_neg_labels= np.array(hard_neg_labels)\r\n",
        "  training_labels = np.append(training_labels, hard_neg_labels, axis=0)\r\n",
        "  print('Done! Training size is: ', training_set.shape[0])\r\n",
        "\r\n",
        "  print(\"Shuffle data...\")\r\n",
        "  (training_set, training_labels) = shuffle_data(training_set, training_labels);\r\n",
        "  training_set = K.cast_to_floatx(training_set)\r\n",
        "  training_labels = K.cast_to_floatx(training_labels)\r\n",
        "\r\n",
        "  filepath=\"/content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_\" + str(n_times-1) +\"-epoch:{epoch:02d}-{loss:.6f}.hdf5\";\r\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1 , mode='min');\r\n",
        "  callbacks_list = [checkpoint, myCallback(\"\")];\r\n",
        "\r\n",
        "  model.fit([training_set[:,0],training_set[:,1]], training_labels[:,0], batch_size=512, epochs=5, callbacks=callbacks_list)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hard negative mining  1 :\n",
            "Embedding reference...\n",
            "Done!\n",
            "Searching...\n",
            "Done\n",
            "Found  355383  hard negative pairs\n",
            "Concatenating hard negative samples to training samples...\n",
            "Done! Training size is:  1567985\n",
            "Shuffle data...\n",
            "Epoch 1/5\n",
            "3063/3063 [==============================] - 693s 226ms/step - loss: 0.3369\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_1-epoch:01-0.336919.hdf5\n",
            "496 538 545 548\n",
            "Average Accuray: 41.030092592592595 %\n",
            "Epoch 2/5\n",
            "3063/3063 [==============================] - 696s 227ms/step - loss: 0.3160\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_1-epoch:02-0.315992.hdf5\n",
            "480 536 539 540\n",
            "Average Accuray: 40.41280864197531 %\n",
            "Epoch 3/5\n",
            "3063/3063 [==============================] - 686s 224ms/step - loss: 0.2945\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_1-epoch:03-0.294493.hdf5\n",
            "487 540 541 542\n",
            "Average Accuray: 40.70216049382716 %\n",
            "Epoch 4/5\n",
            "3063/3063 [==============================] - 666s 218ms/step - loss: 0.3016\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_1-epoch:04-0.301558.hdf5\n",
            "466 513 518 519\n",
            "Average Accuray: 38.88888888888889 %\n",
            "Epoch 5/5\n",
            "3063/3063 [==============================] - 667s 218ms/step - loss: 0.2867\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_1-epoch:05-0.286674.hdf5\n",
            "486 532 533 535\n",
            "Average Accuray: 40.2391975308642 %\n",
            "Hard negative mining  2 :\n",
            "Embedding reference...\n",
            "Done!\n",
            "Searching...\n",
            "Done\n",
            "Found  341122  hard negative pairs\n",
            "Concatenating hard negative samples to training samples...\n",
            "Done! Training size is:  1909107\n",
            "Shuffle data...\n",
            "Epoch 1/5\n",
            "3729/3729 [==============================] - 812s 218ms/step - loss: 0.3132\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/NSEEN_on_CHEBI_weights_model_HNM_2-epoch:01-0.313194.hdf5\n",
            "477 534 538 538\n",
            "Average Accuray: 40.25848765432099 %\n",
            "Epoch 2/5\n",
            "1036/3729 [=======>......................] - ETA: 9:46 - loss: 0.3002"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fAJTm2wr_k5"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\r\n",
        "\r\n"
      ]
    }
  ]
}